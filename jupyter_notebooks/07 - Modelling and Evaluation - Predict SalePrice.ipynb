{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression \n",
    "\n",
    "## objectives \n",
    "\n",
    "\n",
    "* Fit and assess the performance of a regression model aimed at predicting SalePrice levels in a housing market dataset.\n",
    "\n",
    "**Steps to achieve the objective:**\n",
    "\n",
    "* **Data Preparation**\n",
    "\n",
    "* **Feature Engineering**\n",
    "* **Model Selection**\n",
    "* **Model Training**\n",
    "* **Model Evaluation**\n",
    "* **Fine-Tuning**\n",
    "* **Prediction**\n",
    "* **Evaluation**\n",
    "\n",
    "By following these steps, we aim to develop a sturdy regression model that accurately predicts SalePrice levels, providing valuable insights for stakeholders in the housing market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "\n",
    "* outputs/datasets/collection/house-price-2021.csv\n",
    "* Instructions on which variables to use for data cleaning and feature engineering.\n",
    "### Outputs\n",
    "    * Train set (features and target)\n",
    "    * Test set (features and target)\n",
    "    * ML pipeline to predict tenure\n",
    "    * labels map\n",
    "    * Feature Importance Plot\n",
    "---\n",
    "## Change working directory\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "\n",
    "    * os.path.dirname() gets the parent directory\n",
    "    * os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "df_reg = (pd.read_csv(\"/workspace/housing-market-analysis/outputs/datasets/collection/house-price-2021.csv\")\n",
    "     .drop(labels=['WoodDeckSF', 'EnclosedPorch'], axis=1))\n",
    "df_reg['GarageYrBlt'] = df_reg['GarageYrBlt'].astype(int)\n",
    "print(df_reg.shape)\n",
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_columns = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
    "\n",
    "# Create an instance of OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Apply Ordinal Encoding to the selected categorical columns\n",
    "df_reg[categorical_columns] = ordinal_encoder.fit_transform(df_reg[categorical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split Train Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                    df_reg.drop(['SalePrice'], axis=1),\n",
    "                                    df_reg['SalePrice'],\n",
    "                                    test_size=0.2,\n",
    "                                    random_state=101\n",
    ")\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\", X_test.shape, y_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model, Pipeline: Regressor**\n",
    "\n",
    "**Create a ML pipeline for Data Cleaning and Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Feature Engineering\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "# Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithms\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        ('median', MeanMedianImputer()),\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "         method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "\n",
    "        (\"feat_scaling\", StandardScaler()),\n",
    "\n",
    "        (\"feat_selection\",  SelectFromModel(model)),\n",
    "\n",
    "        (\"model\", model),\n",
    "\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML Pipeline for Modellng and Hyperparameter Optimisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model = PipelineOptimization(self.models[key])\n",
    "\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grid Search CV - Sklearn** \n",
    "\n",
    "**Use default hyperparameters to find most suitable algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=101),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=101),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=101),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=101),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=101),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=101)\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a hyperparameter optimisatiion search using default hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='r2',\n",
    "           n_jobs=-1, # use all processors, but one\n",
    "           cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check hyperparameter optimisation search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In which algorithms would you spend time doing an extensive hyperparameter search?**\n",
    "\n",
    "* The top three performers performed the best.\n",
    "* I would certainly select ExtraTreesClassifier and would give a second chance to RandomForestClassifier and AdaBoostRegressor, since its performance was not so far from ExtraTress.\n",
    "* I wouldn't give a second chance to the rest of the algorithms as it is quite far off from the top Three algorithms.\n",
    "\n",
    "**Let's define the new hyperparameters for the extensive search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=101),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=101),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=101),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"ExtraTreesRegressor\": {\"model__n_estimators\": [100, 150, 200]},\n",
    "    \"RandomForestRegressor\": {\"model__n_estimators\": [100, 150, 170]},\n",
    "    \"AdaBoostRegressor\": {\"model__n_estimators\": [50, 100, 120]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I fit again using our HyperparameterOptimizationSearch class and our updated information on models_search and params_search.\n",
    "\n",
    "* The objective is to conduct an exhaustive search on the algorithms that performed better than default hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring='r2',\n",
    "           n_jobs=-1,\n",
    "           cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the results summary with .score_summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the best model name, I done this by using .iloc[ ] on the first row and column from the previous data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll get the best model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_pipelines[best_model].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable grid_search_pipelines holds all the trained pipelines. Start by selecting the pipelines from the model that performed the best (with best_model), then use .best_estimator_ to get the pipeline that has the model and hyperparameter settings that work best with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data cleaning and feature engine**\n",
    "\n",
    "Transforming the training data using the pipeline defined by \"best_pipeline\", extracting the columns after the data cleaning and feature engineering steps, then identifying the best features selected by the feature selection step, finally displaying the feature importance using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# There are 2 data cleaning and feature engineering steps: median and standard scaler\n",
    "\n",
    "data_cleaning_feat_eng_steps = 2\n",
    "# we get these steps with .steps[] starting from 0 until the value we assigned above\n",
    "# then we .transform() to the train set and extract the columns\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_pipeline.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "# we get the boolean list indicating the best features with best_pipeline['feat_selection'].get_support()\n",
    "# and use this list to sbuset columns_after_data_cleaning_feat_eng\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_pipeline['feat_selection'].get_support()].to_list()\n",
    "\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': best_features,\n",
    "          'Importance': best_pipeline['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order.\"\n",
    "      f\" The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the pipeline using hyperparameter optimization, you'll want to evaluate its performance. This involves testing it on unseen data (test set) and analyzing various metrics such as **mean squared error**, **mean absolute error**, **R-squared**, **r2** and others. To observe how well the model generalizes to new unseen data. Moreover, To assist with this evaluation, we'll import a custom function designed specifically for assessing regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "import numpy as np\n",
    "\n",
    "def regression_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "\tprint(\"Model Evaluation \\n\")\n",
    "\tprint(\"* Train Set\")\n",
    "\tregression_evaluation(X_train,y_train,pipeline)\n",
    "\tprint(\"* Test Set\")\n",
    "\tregression_evaluation(X_test,y_test,pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  \n",
    "\n",
    "def regression_evaluation_plots(X_train, y_train, X_test, y_test, pipeline, alpha_scatter=0.5):\n",
    "  pred_train = pipeline.predict(X_train)\n",
    "  pred_test = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=y_train , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=y_train , y=y_train, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=y_test, y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=y_test , y=y_test, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model performs exceptionally well on the training set, achieving a perfect R2 score and minimal errors. However, on the test set, while the R2 score is still quite good at 0.742, there are significant errors in terms of mean absolute error, mean squared error, and root mean squared error. This indicates that while the model generalizes well, it may be overfitting to some extent and could benefit from further optimization or regularization to improve its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test,best_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test, \n",
    "                            best_pipeline, alpha_scatter=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Regressor with PCA**\n",
    "Let's explore potential values for PCA n_components.\n",
    "\n",
    "In the next cells we are going to:\n",
    "\n",
    "        * Transform the data using PCA and understand how many components to consider\n",
    "        * Visualize the data after the PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd \n",
    "\n",
    "df_pca = pd.read_csv(\"/workspace/housing-market-analysis/outputs/datasets/collection/house-price-2021.csv\")\n",
    "df_pca = df_pca.sample(frac=0.8, random_state=101)\n",
    "df_pca = df_pca.drop(labels=['WoodDeckSF', 'EnclosedPorch'], axis=1) # Drop columns 'WoodDeckSF' and 'EnclosedPorch'\n",
    "# df_pca['GarageYrBlt'] = df_pca['GarageYrBlt'].astype(int)\n",
    "print(df_pca.shape)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = df_pca[['SalePrice']]\n",
    "X = df_pca.drop(['SalePrice'], axis=1)\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_columns = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
    "\n",
    "# Create an instance of OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Apply Ordinal Encoding to the selected categorical columns\n",
    "df_pca[categorical_columns] = ordinal_encoder.fit_transform(df_pca[categorical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline steps\n",
    "\n",
    "To use PCA, it's important to scale the data first. Hence, we're setting up a pipeline to handle data preparation, feature engineering, and scaling.\n",
    "\n",
    "In our setup, this pipeline will handle data cleaning by filling in missing values using median imputation, and then it will scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "### Data Cleaning\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "### Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def PipelineDataCleaningFeatEngFeatScaling():\n",
    "  pipeline_base = Pipeline([\n",
    "                            \n",
    "      ( 'MeanMedianImputer', MeanMedianImputer(imputation_method='median') ),\n",
    "\n",
    "      ( 'feature_scaling', StandardScaler() ),\n",
    "  ])\n",
    "\n",
    "  return pipeline_base\n",
    "\n",
    "PipelineDataCleaningFeatEngFeatScaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to apply PCA solely to the features, excluding the SalePrice. We create two separate DataFrames: X, containing the features, and df_target, which holds the SalePrice information the capital range. It's important to note that X comprises 21 features. We will utilize X for PCA application and df_target at a subsequent stage when visualizing the data.\n",
    "\n",
    "Apply PCA separately to the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 21 # number of components as all columns in the data\n",
    "\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)  # set PCA object and fit to the data\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "\n",
    "# the Principal Component Analysis object has .explained_variance_ratio_ attribute, which tells \n",
    "# how much variance each component has \n",
    "# We store that to a DataFrame relating each component to its variance explanation\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ ,4),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "# prints how much of the dataset these components explain\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "print(dfExplVarRatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we just copied the code from the cell above and changed n_components to 2.\n",
    "\n",
    " - With 2 components we achieved 99.98\n",
    "\n",
    "Overall, this indicates that the first two components contribute significantly to explaining the variability in the data, while the remaining components contribute very little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5\n",
    "\n",
    "pca = PCA(n_components=n_components).fit(df_pca)\n",
    "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
    "\n",
    "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
    "dfExplVarRatio = pd.DataFrame(\n",
    "    data= np.round(100 * pca.explained_variance_ratio_ , 4),\n",
    "    index=ComponentsList,\n",
    "    columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
    "\n",
    "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained, 4)}% of the data \\n\")\n",
    "print(dfExplVarRatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x_PCA.shape method returns the shape of the transformed PCA data array, which represents the data after applying principal component analysis (PCA). The shape is a tuple indicating the dimensions of the array, where the first element represents the number of samples or rows and the second element represents the number of features or components or columns in the numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_PCA.shape)\n",
    "x_PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite ML Pipeline for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "\n",
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "\n",
    "        (\"median_imputer\", MeanMedianImputer(imputation_method='median')),\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "         method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "\n",
    "\n",
    "        (\"feat_scaling\", StandardScaler()),\n",
    "\n",
    "        # PCA replace Feature Selection\n",
    "        (\"PCA\", PCA(n_components=7, random_state=0)),\n",
    "\n",
    "        (\"model\", model),\n",
    "\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV – Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"* Train set:\", X_train.shape, y_train.shape, \"\\n* Test set:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use standard hyperparameters to find the most suitable model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=101),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=101),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=101),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=101),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=101),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=101),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "quick_search.fit(X_train, y_train, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do an extensive search on the most suitable model to find the best hyperparameter configuration**.\n",
    "\n",
    "Define model and parameters for extensive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"GradientBoostingRegressor\":GradientBoostingRegressor(random_state=101),\n",
    "}\n",
    "\n",
    "\n",
    "params_search = {\n",
    "    \"GradientBoostingRegressor\":{\n",
    "        'model__n_estimators': [100,150,200],\n",
    "        'model__learning_rate': [1,2,3], \n",
    "        'model__max_depth': [3,5,7], # Limits the number of nodes in the tree\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train, scoring = 'r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_pipelines[best_model].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the best regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_pipeline = grid_search_pipelines[best_model].best_estimator_\n",
    "best_regressor_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluate Regressor on Train and Tests Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(X_train, y_train, X_test, y_test,best_regressor_pipeline)\n",
    "regression_evaluation_plots(X_train, y_train, X_test, y_test,\n",
    "                            best_regressor_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convert Regression to Classification**\n",
    "### Convert numerical target to bins, and check if it is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "disc = EqualFrequencyDiscretiser(q=4, variables=['OverallQual'])  # we will try q as 3, 4, and 7\n",
    "df_clf = disc.fit_transform(df_pca)\n",
    "\n",
    "print(f\"* The classes represent the following ranges: \\n{disc.binner_dict_} \\n\")\n",
    "sns.countplot(data=df_clf, x='OverallQual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite ML Pipeline for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineOptimization(model):\n",
    "    pipeline_base = Pipeline([\n",
    "\n",
    "        (\"OrdinalCategoricalEncoder\", OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "\n",
    "\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "         method=\"spearman\", threshold=0.6, selection_method=\"variance\")),\n",
    "\n",
    "        (\"feat_scaling\", StandardScaler()),\n",
    "\n",
    "        (\"feat_selection\",  SelectFromModel(model)),\n",
    "\n",
    "        (\"model\", model),\n",
    "\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load algorithms for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['OverallQual'], axis=1),\n",
    "    df_clf['OverallQual'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"* Train set:\", X_train.shape, y_train.shape,\n",
    "      \"\\n* Test set:\",  X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Seach CV – Sklearn\n",
    "### Use standard hyper parameters to find most suitable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"XGBClassifier\": XGBClassifier(random_state=101),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=101),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=101),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=101),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(random_state=101),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=101, algorithm='SAMME'),\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"XGBClassifier\":{},\n",
    "    \"DecisionTreeClassifier\":{},\n",
    "    \"RandomForestClassifier\":{},\n",
    "    \"GradientBoostingClassifier\":{},\n",
    "    \"ExtraTreesClassifier\":{},\n",
    "    \"AdaBoostClassifier\":{},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, recall_score\n",
    "quick_search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "quick_search.fit(X_train, y_train,\n",
    "                 scoring = make_scorer(recall_score, labels=[0], average=None),\n",
    "                 n_jobs=-1,\n",
    "                 cv=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do an extensive search on the most suitable model to find the best hyperparameter configuration.\n",
    "\n",
    "Define models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(random_state=101, algorithm='SAMME'),\n",
    "}\n",
    "\n",
    "# documentation to help on hyperparameter list:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "params_search = {\n",
    "    \"AdaBoostClassifier\": {\n",
    "        'model__n_estimators': [50, 100, 150],\n",
    "        'model__learning_rate': [0.1, 0.01, 0.001],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer,  recall_score\n",
    "search = HyperparameterOptimizationSearch(\n",
    "    models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,\n",
    "           scoring=make_scorer(recall_score, labels=[0], average=None),\n",
    "           n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for best model\n",
    "\n",
    " - We are saving this content for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the best clf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
    "pipeline_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe feature importance\n",
    "We can assess feature importance for this model with .feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_feat_eng_steps = 2\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "# best_features = columns_after_data_cleaning_feat_eng\n",
    "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf['feat_selection'].get_support(\n",
    ")].to_list()\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf['feat_selection'].get_support()],\n",
    "    'Importance': pipeline_clf['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "# reassign best features in order\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{best_features}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', xlabel='Feature', ylabel='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Classifier on Train and Test Sets\n",
    "\n",
    "Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
    "\n",
    "    prediction = pipeline.predict(X)\n",
    "\n",
    "    print('---  Confusion Matrix  ---')\n",
    "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
    "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
    "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
    "          ))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print('---  Classification Report  ---')\n",
    "    print(classification_report(y, prediction, target_names=label_map, zero_division=0), \"\\n\")\n",
    "\n",
    "\n",
    "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
    "    print(\"#### Train Set #### \\n\")\n",
    "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
    "\n",
    "    print(\"#### Test Set ####\\n\")\n",
    "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List that relates the classes and OverallQual interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.binner_dict_['OverallQual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mapping can be useful for tasks such as discretization, classification, or data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = ['<5', '5 to 6','7', '+7']\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_performance(X_train=X_train, y_train=y_train,\n",
    "                        X_test=X_test, y_test=y_test,\n",
    "                        pipeline=pipeline_clf,\n",
    "                        label_map= label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push files to the repo\n",
    "We will generate the following files\n",
    "\n",
    "- Train set\n",
    "- Test set\n",
    "- Modeling pipeline\n",
    "- label map\n",
    "- features importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_pipeline/predict_OverallQual/{version}'\n",
    "\n",
    "try:\n",
    "  os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train Set: features and target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set: features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List mapping target levels to ranges\n",
    "Map for converting numerical variable to categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=label_map, filename=f\"{file_path}/label_map.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', xlabel='Feature', ylabel='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet generates a bar plot using pandas to visualize the importance of the *features* in a dataset, with *features* represented on the x-axis and their respective importance values on the y-axis. It then saves the plot as an image file named \"features_importance.png\" in a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', xlabel='Feature', ylabel='Importance')\n",
    "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a scatter plot to help us understand how two specific features, **'OverallQual'** and **'GrLivArea'**, relate to the target variable **'SalePrice'**. By plotting **'OverallQual'** on the x-axis and **'GrLivArea'** on the y-axis, each data point represents a combination of these two features, and its color indicates the corresponding **'SalePrice'**. This visualization allows us to explore potential patterns or correlations between these features and the target variable. \n",
    "The larger the ground living area and the higher the over all quality of the house, the higher the SalePrice/cost of the housing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1, var2 = 'OverallQual' , 'GrLivArea'\n",
    "sns.scatterplot(x=X[var1], y=X[var2], hue=df_SP['SalePrice'])\n",
    "sns.despine()\n",
    "plt.xlabel(var1)\n",
    "plt.ylabel(var2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting a scatter plot using the first two principal components (Component 0 and Component 1) generated by PCA. The x-coordinate represents the values of Component 0, while the y-coordinate represents the values of Component 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_PCA[:,0], y=x_PCA[:,1], hue=df_SP['SalePrice'], alpha=0.8)\n",
    "plt.xlabel('Component-0')\n",
    "plt.ylabel('Component-1')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "used Plotly Express to create a 3D scatter plot. The x, y, and z axes represent the first three principal components obtained from PCA. Each point in the plot corresponds to a data sample, and its color indicates the SalePrice label (e.g, the capital range). This visualization helps us explore the distribution of samples in the three-dimensional space defined by the principal components and understand how they are separated or clustered based on their price labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_3d(x=x_PCA[:,0], y=x_PCA[:,1], z= x_PCA[:,2] , color=df_SP['SalePrice'],\n",
    "                    labels=dict(x=\"Component 0\", y=\"Component 1\", z='Component 2'),\n",
    "                    color_continuous_scale='RdYlBu',\n",
    "                    width=750, height=500)\n",
    "fig.update_traces(marker_size=5)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
