{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster\n",
    "### Objectives\n",
    "- Fit and evaluate a cluster model to group similar data\n",
    "- Understand the profile for each cluster\n",
    "### Inputs\n",
    "- outputs/datasets/collection/house-price-2021.csv\n",
    "- Instructions on which variables to use for data cleaning and feature engineering. They are found in their respective notebooks.\n",
    "### Outputs\n",
    "- Cluster Pipeline\n",
    "- Train Set\n",
    "- Most important features to define a cluster plot\n",
    "- Clusters Profile Description\n",
    "- Cluster Silhouette\n",
    "---\n",
    "## Change working directory\n",
    "\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "\n",
    "- We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the parent of current directory the new current directory\n",
    "\n",
    "    - os.path.dirname() gets the parent directory\n",
    "    - os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/collection/house-price-2021.csv\")\n",
    "      .drop(['WoodDeckSF', 'SalePrice', 'EnclosedPorch'], axis=1)\n",
    "      )\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Pipeline with all data\n",
    "### ML Cluster Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "from feature_engine.transformation import LogTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr',\n",
    "                    'BsmtFinSF1', 'BsmtUnfSF','LotArea',\n",
    "                    'GarageArea',  'GarageYrBlt', 'OpenPorchSF',\n",
    "                    'LotFrontage', 'MasVnrArea',  'OverallCond',\t\n",
    "                    'OverallQual', 'TotalBsmtSF', 'YearBuilt', 'YearRemodAdd']  # List of numerical features\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = ['KitchenQual','BsmtExposure','BsmtFinType1','GarageFinish']  # List of categorical features\n",
    "\n",
    "def PipelineCluster():\n",
    "    # Define ordinal encoder for categorical features\n",
    "    ordinal_encoder = OrdinalEncoder(encoding_method='arbitrary', variables=categorical_features)\n",
    "\n",
    "    # Define correlated feature selection\n",
    "    correlated_selection = SmartCorrelatedSelection(method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "\n",
    "    # Define imputer for missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Define scaler for numerical features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=50, random_state=101)\n",
    "\n",
    "    # Define KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=50, random_state=101)\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", ordinal_encoder),\n",
    "        (\"SmartCorrelatedSelection\", correlated_selection),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"PCA\", pca),\n",
    "        (\"model\", kmeans)\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
    "df_pca = pipeline_pca.fit_transform(df)\n",
    "\n",
    "print(df_pca.shape,'\\n', type(df_pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA separately to the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "n_components = 15\n",
    "\n",
    "\n",
    "def pca_components_analysis(df_pca, n_components):\n",
    "    pca = PCA(n_components=n_components).fit(df_pca)\n",
    "    x_PCA = pca.transform(df_pca)  # array with transformed PCA\n",
    "\n",
    "    ComponentsList = [\"Component \" + str(number)\n",
    "                      for number in range(n_components)]\n",
    "    dfExplVarRatio = pd.DataFrame(\n",
    "        data=np.round(100 * pca.explained_variance_ratio_, 3),\n",
    "        index=ComponentsList,\n",
    "        columns=['Explained Variance Ratio (%)'])\n",
    "\n",
    "    dfExplVarRatio['Accumulated Variance'] = dfExplVarRatio['Explained Variance Ratio (%)'].cumsum(\n",
    "    )\n",
    "\n",
    "    PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum(\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.lineplot(data=dfExplVarRatio,  marker=\"D\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(np.arange(0, 110, 10))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pca_components_analysis(df_pca=df_pca, n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components_analysis(df_pca=df_pca, n_components=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    # Define ordinal encoder for categorical features\n",
    "    ordinal_encoder = OrdinalEncoder(encoding_method='arbitrary', variables=categorical_features)\n",
    "\n",
    "    # Define correlated feature selection\n",
    "    correlated_selection = SmartCorrelatedSelection(method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "\n",
    "    # Define imputer for missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Define scaler for numerical features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=9, random_state=101)\n",
    "\n",
    "    # Define KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=50, random_state=101)\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", ordinal_encoder),\n",
    "        (\"SmartCorrelatedSelection\", correlated_selection),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"PCA\", pca),\n",
    "        (\"model\", kmeans)\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method and Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(df)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Available font families\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=101), k=(1,11)) # 11 is not included, it will plot until 10\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# 9 is not included, it will stop at 8\n",
    "n_cluster_start, n_cluster_stop = 3, 9\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=101), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=101),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineCluster():\n",
    "    # Define ordinal encoder for categorical features\n",
    "    ordinal_encoder = OrdinalEncoder(encoding_method='arbitrary', variables=categorical_features)\n",
    "\n",
    "    # Define correlated feature selection\n",
    "    correlated_selection = SmartCorrelatedSelection(method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "\n",
    "    # Define imputer for missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Define scaler for numerical features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=8, random_state=101)\n",
    "\n",
    "    # Define KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=3, random_state=101)\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", ordinal_encoder),\n",
    "        (\"SmartCorrelatedSelection\", correlated_selection),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"PCA\", pca),\n",
    "        (\"model\", kmeans)\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Cluster Pipeline\n",
    "Quick recap of our data for training cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cluster predictions to dataset\n",
    "We add a column *Clusters* (with the cluster pipeline predictions) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=df_analysis[:, 0], y=df_analysis[:, 1],\n",
    "                hue=X['Clusters'], palette='bright', alpha=0.6)\n",
    "plt.scatter(x=pipeline_cluster['model'].cluster_centers_[:, 0], y=pipeline_cluster['model'].cluster_centers_[:, 1],\n",
    "            marker=\"*\", s=140, linewidths=1, color=\"black\")\n",
    "plt.xlabel(\"PCA Component 0\")\n",
    "plt.ylabel(\"PCA Component 1\")\n",
    "plt.title(\"PCA Components colored by Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the cluster predictions from this pipeline to use in the future. We will get back to that in a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables = X['Clusters']\n",
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a classifier, where the target is cluster predictions and features remaining variables\n",
    "We copy X to a DataFrame df_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create classifier pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "def PipelineClf2ExplainClusters():\n",
    "    # Define ordinal encoder for categorical features\n",
    "    ordinal_encoder = OrdinalEncoder(encoding_method='arbitrary', variables=categorical_features)\n",
    "\n",
    "    # Define correlated feature selection\n",
    "    correlated_selection = SmartCorrelatedSelection(method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
    "\n",
    "    # Define scaler for numerical features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define feature selection using GradientBoostingClassifier\n",
    "    feature_selection = SelectFromModel(GradientBoostingClassifier(random_state=101))\n",
    "\n",
    "    # Define GradientBoostingClassifier\n",
    "    classifier = GradientBoostingClassifier(random_state=101)\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", ordinal_encoder),\n",
    "        (\"SmartCorrelatedSelection\", correlated_selection),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"imputer\", imputer),\n",
    "        (\"feat_selection\", feature_selection),\n",
    "        (\"model\", classifier)\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base\n",
    "\n",
    "\n",
    "PipelineClf2ExplainClusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the classifier to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check to observe if there are any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum(), y_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate classifier performance on the Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *classification_report()* function in scikit-learn generates a comprehensive report of classification metrics, such as precision, recall, F1-score, and support, for each class in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the most important Features in the dataset that define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after data cleaning and feature engineering, the feature space changes\n",
    "# This pipeline has two data cleaning and feature engineering steps \n",
    "\n",
    "data_cleaning_feat_eng_steps = 2\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(X_train)\n",
    "                                        .columns)\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support(\n",
    ")].to_list()\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
    "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "# reassign best features in importance order\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best_features are listed below and are stored in the variable *best_features_pipeline_all_variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables = best_features\n",
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "Load function that plots a table with description for all Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DescriptionAllClusters(df, decimal_points=3):\n",
    "\n",
    "    DescriptionAllClusters = pd.DataFrame(\n",
    "        columns=df.drop(['Clusters'], axis=1).columns)\n",
    "    # iterate on each cluster , calls Clusters_IndividualDescription()\n",
    "    for cluster in df.sort_values(by='Clusters')['Clusters'].unique():\n",
    "\n",
    "        EDA_ClusterSubset = df.query(\n",
    "            f\"Clusters == {cluster}\").drop(['Clusters'], axis=1)\n",
    "        ClusterDescription = Clusters_IndividualDescription(\n",
    "            EDA_ClusterSubset, cluster, decimal_points)\n",
    "        DescriptionAllClusters = pd.concat([DescriptionAllClusters, ClusterDescription])\n",
    "\n",
    "    DescriptionAllClusters.set_index(['Cluster'], inplace=True)\n",
    "    return DescriptionAllClusters\n",
    "\n",
    "\n",
    "def Clusters_IndividualDescription(EDA_Cluster, cluster, decimal_points):\n",
    "\n",
    "    ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
    "    # for a given cluster, iterate over all columns\n",
    "    # if the variable is numerical, calculate the IQR: display as Q1 -- Q3.\n",
    "    # That will show the range for the most common values for the numerical variable\n",
    "    # if the variable is categorical, count the frequencies and displays the top 3 most frequent\n",
    "    # That will show the most common levels for the category\n",
    "\n",
    "    for col in EDA_Cluster.columns:\n",
    "\n",
    "        try:  # eventually a given cluster will have only missing data for a given variable\n",
    "\n",
    "            if EDA_Cluster[col].dtypes == 'object':\n",
    "\n",
    "                top_frequencies = EDA_Cluster.dropna(\n",
    "                    subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
    "                Description = ''\n",
    "\n",
    "                for x in range(len(top_frequencies)):\n",
    "                    freq = top_frequencies.iloc[x]\n",
    "                    category = top_frequencies.index[x][0]\n",
    "                    CategoryPercentage = int(round(freq*100, 0))\n",
    "                    statement = f\"'{category}': {CategoryPercentage}% , \"\n",
    "                    Description = Description + statement\n",
    "\n",
    "                ClustersDescription.at[0, col] = Description[:-2]\n",
    "\n",
    "            elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
    "                DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
    "                Q1 = round(DescStats.iloc[4, 0], decimal_points)\n",
    "                Q3 = round(DescStats.iloc[6, 0], decimal_points)\n",
    "                Description = f\"{Q1} -- {Q3}\"\n",
    "                ClustersDescription.at[0, col] = Description\n",
    "\n",
    "        except Exception as e:\n",
    "            ClustersDescription.at[0, col] = 'Not available'\n",
    "            print(\n",
    "                f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
    "\n",
    "    ClustersDescription['Cluster'] = str(cluster)\n",
    "\n",
    "    return ClustersDescription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load custom function to plot cluster distribution per Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def cluster_distribution_per_variable(df, target):\n",
    "    \"\"\"\n",
    "    The data should have 2 variables, the cluster predictions and\n",
    "    the variable you want to analyze with, in this case we call \"target\".\n",
    "    We use plotly express to create 2 plots:\n",
    "    Cluster distribution across the target.\n",
    "    Relative presence of the target level in each cluster.\n",
    "    \"\"\"\n",
    "    df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index()\n",
    "    df_bar_plot.columns = ['Clusters', target, 'Count']\n",
    "    df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
    "\n",
    "    print(f\"Clusters distribution across {target} levels\")\n",
    "    fig = px.histogram(df_bar_plot, x='Clusters', y='Count',\n",
    "                 color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.show(renderer='colab')\n",
    "\n",
    "    df_relative = (df\n",
    "                   .groupby([\"Clusters\", target])\n",
    "                   .size()\n",
    "                   .unstack()\n",
    "                   .groupby(level=0)\n",
    "                   .apply(lambda x:  100*x / x.sum())\n",
    "                   .stack()\n",
    "                   .reset_index(name='Relative Percentage (%)')\n",
    "                   .sort_values(by=['Clusters'])\n",
    "                   )\n",
    "    df_relative.columns = ['Clusters', target, 'Relative Percentage (%)']\n",
    "\n",
    "    print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
    "    fig = px.line(df_relative, x='Clusters', y='Relative Percentage (%)',\n",
    "                  color=target, width=800, height=500)\n",
    "    fig.update_layout(xaxis=dict(tickmode='array',\n",
    "                      tickvals=df['Clusters'].unique()))\n",
    "    fig.update_traces(mode='markers+lines')\n",
    "    fig.show(renderer='colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame that contains best features and Clusters Predictions since we want to analyse the patterns for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "print(df_cluster_profile.shape)\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want also to analyse SalePrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = pd.read_csv(\"outputs/datasets/collection/house-price-2021.csv\").filter(['SalePrice'])\n",
    "df_SP['SalePrice'] = df_SP['SalePrice'] \n",
    "df_SP.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster profile based on the best features\n",
    "\n",
    "This represents the range of values for various features within each cluster, including the years built, basement finished square footage, unfinished square footage, total basement square footage, garage finish type distribution (RFn, Unf, Fin), and sale prices, with corresponding percentages for each category.\n",
    "\n",
    "**Cluster 0:** represents houses between 1976 - 2005, with basement finished square footage ranging from 0.0 to 528.0, unfinished square footage ranging from 474.0 to 1252.0, total basement square footage ranging from 840.0 to 1405.0, a garage finish distribution of 'RFn': 54%, 'Unf': 28%, and 'Fin': 18%. Finally Sale prices fall between 160000.0 and 214000.0.\n",
    "\n",
    "**Cluster 1:** Has houses between 1996 - 2006, with basement finished square footage ranging from 759.0 to 1218.0, unfinished square footage ranging from 116.0 to 438.0, total basement square footage ranging from 1054.0 to 1639.0, a garage finish distribution of 'Fin': 68%, 'Unf': 24%, and 'RFn': 8%. Finally Sale prices fall between 195575.0 and 301875.0.\n",
    "\n",
    "**Cluster 2:** showcases properties between 1922 - 1965, with basement finished square footage ranging from 0.0 to 539.0, unfinished square footage ranging from 168.0 to 684.0, total basement square footage ranging from 684.0 to 1003.0, a garage finish distribution of 'Unf': 70%, 'None': 11%, and 'Fin': 10%. Finally Sale prices fall between 110500.0 and 155000.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(df=pd.concat([df_cluster_profile, df_SP], axis=1), decimal_points=0)\n",
    "clusters_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of df_cluster_profile\n",
    "print(\"First few rows of df_cluster_profile:\")\n",
    "print(df_cluster_profile.head())\n",
    "\n",
    "# Display the shape of df_cluster_profile (number of rows and columns)\n",
    "print(\"\\nShape of df_cluster_profile:\")\n",
    "print(df_cluster_profile.shape)\n",
    "\n",
    "# Display information about df_cluster_profile, including data types and missing values\n",
    "print(\"\\nInformation about df_cluster_profile:\")\n",
    "print(df_cluster_profile.info())\n",
    "\n",
    "# Display summary statistics for numeric columns in df_cluster_profile\n",
    "print(\"\\nSummary statistics for df_cluster_profile:\")\n",
    "print(df_cluster_profile.describe())\n",
    "\n",
    "# Repeat the above steps for df_SP\n",
    "print(\"\\nFirst few rows of df_SP:\")\n",
    "print(df_SP.head())\n",
    "\n",
    "print(\"\\nShape of df_SP:\")\n",
    "print(df_SP.shape)\n",
    "\n",
    "print(\"\\nInformation about df_SP:\")\n",
    "print(df_SP.info())\n",
    "\n",
    "print(\"\\nSummary statistics for df_SP:\")\n",
    "print(df_SP.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters distribution across Churn levels & Relative Percentage of SalePrice in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_vs_SP = df_SP.copy()\n",
    "df_cluster_vs_SP['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_SP, target='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit New Cluster Pipeline with most important features\n",
    "In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline using the variables that are most important to define the clusters from the previous pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_pipeline_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define trade-off and metrics to compare new and previous Cluster Pipeline\n",
    "To evaluate this trade-off we will\n",
    "\n",
    "1. Conduct a elbow method and silhouette analysis and check if the same number of clusters is suggested\n",
    "2. Fit new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline\n",
    "3. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\n",
    "4. Check if the most important features for the classifier are the same from the previous pipeline\n",
    "5. ompare if the cluster profile from both pipelines are \"equivalent\"\n",
    "\n",
    "If we are happy to say **yes** for them, we can use a cluster pipeline using the features that best define the clusters from previous pipeline!\n",
    "\n",
    "* The **gain** is that in real-time (which is the major purpose of Machine Learning) you will need fewer variables for predicting clusters for your prospects.\n",
    "\n",
    "### Subset data with the most relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite Cluster Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def PipelineCluster():\n",
    "    # Define numerical and categorical features\n",
    "    numerical_features = ['YearBuilt', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF'] \n",
    "    categorical_features = ['GarageFinish']  \n",
    "\n",
    "    # Define transformers for numerical and categorical features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Combine numerical and categorical transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numerical', numeric_transformer, numerical_features),\n",
    "            ('categorical', categorical_transformer, categorical_features)])\n",
    "\n",
    "    # Define KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_base = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', kmeans)\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = PipelineCluster()\n",
    "\n",
    "# Display the pipeline\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Elbow Method and Silhouette analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
    "df_analysis = pipeline_analysis.fit_transform(df_reduced)\n",
    "\n",
    "print(df_analysis.shape,'\\n', type(df_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
    "visualizer.fit(df_analysis) \n",
    "visualizer.show() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code visualizes the silhouette scores and plots for different numbers of clusters to aid in selecting the optimal number of clusters for KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "n_cluster_start, n_cluster_stop = 2, 5\n",
    "\n",
    "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(\n",
    "    n_cluster_start, n_cluster_stop), metric='silhouette')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "for n_clusters in np.arange(start=n_cluster_start, stop=n_cluster_stop):\n",
    "\n",
    "    print(f\"=== Silhouette plot for {n_clusters} Clusters ===\")\n",
    "    visualizer = SilhouetteVisualizer(estimator=KMeans(n_clusters=n_clusters, random_state=0),\n",
    "                                      colors='yellowbrick')\n",
    "    visualizer.fit(df_analysis)\n",
    "    visualizer.show()\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit New Cluster Pipeline\n",
    "We set X as our training set for the cluster. It is a copy of df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced.copy()\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster = PipelineCluster()\n",
    "pipeline_cluster.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add cluster predictions to dataset\n",
    "We add a column \"*Clusters*\" (with the cluster pipeline predictions) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Clusters'] = pipeline_cluster['model'].labels_\n",
    "print(X.shape)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
    "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare current cluster predictions to previous cluster predictions\n",
    "We just fitted a new cluster pipeline and want to compare if its predictions are \"*equivalent*\" to the previous cluster.\n",
    "\n",
    "These are the predictions from the **previous** cluster pipeline - trained with all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the predictions from **current** cluster pipeline (trained with ['OnlineBackup', 'MonthlyCharges', 'PhoneService'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_predictions_with_best_features = X['Clusters'] \n",
    "cluster_predictions_with_best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a confusion matrix to evaluate if the predictions of both pipelines are \"**equivalent**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(cluster_predictions_with_all_variables, cluster_predictions_with_best_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a classifier, where the target is cluster predictions and features remaining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = X.copy()\n",
    "print(df_clf.shape)\n",
    "df_clf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_clf.drop(['Clusters'], axis=1),\n",
    "    df_clf['Clusters'],\n",
    "    test_size=0.2,\n",
    "    random_state=101\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "df_clf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite pipeline to explain clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def PipelineClf2ExplainClusters():\n",
    "    # Define categorical features\n",
    "    categorical_features = ['GarageFinish']\n",
    "\n",
    "    # Define transformers for categorical features\n",
    "    categorical_transformer = OrdinalEncoder(categories='auto', handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "    # Define scaler for numerical features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Define GradientBoostingClassifier\n",
    "    classifier = GradientBoostingClassifier(random_state=101)\n",
    "\n",
    "    # Define pipeline steps\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"OrdinalCategoricalEncoder\", categorical_transformer),\n",
    "        (\"scaler\", scaler),\n",
    "        (\"model\", classifier)\n",
    "    ])\n",
    "    \n",
    "    return pipeline_base\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a classifier, where the target is cluster labels and features remaining variables\n",
    "\n",
    "Create and fit a classifier pipeline to learn the feature importance when defining a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
    "pipeline_clf_cluster.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate classifier performance on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we don't have feature selection step in this pipeline, best_features is X_train columns\n",
    "best_features = X_train.columns.to_list()\n",
    "\n",
    "# create a DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': best_features,\n",
    "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Analysis\n",
    "Create a DataFrame that contains the best features and Clusters Predictions: we want to analyse the patterns for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_profile = df_clf.copy()\n",
    "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
    "df_cluster_profile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want also to analyse SalePrice levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = pd.read_csv(\"outputs/datasets/collection/house-price-2021.csv\").filter(['SalePrice'])\n",
    "df_SP['SalePrice'] = df_SP['SalePrice']\n",
    "df_SP.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster profile on most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "clusters_profile = DescriptionAllClusters(df= pd.concat([df_cluster_profile, df_SP], axis=1), decimal_points=0)\n",
    "clusters_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters distribution across SalePrice levels & Relative Percentage of SalePrice in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Generate the plot\n",
    "df_cluster_vs_SP = df_SP.copy()\n",
    "df_cluster_vs_SP['Clusters'] = X['Clusters']\n",
    "cluster_distribution_per_variable(df=df_cluster_vs_SP, target='SalePrice')\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('cluster_distribution.png')\n",
    "\n",
    "# Display the saved image\n",
    "Image(filename='cluster_distribution.png')\n",
    "image_path = 'cluster_distribution.png'\n",
    "display(Image(filename=image_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push files to Repo\n",
    "We will generate the following files\n",
    "\n",
    "* Cluster Pipeline\n",
    "* Train Set\n",
    "* Feature importance plot\n",
    "* Clusters Description\n",
    "* Cluster Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
    "\n",
    "try:\n",
    "    os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reduced.shape)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important features plot\n",
    "These are the features that define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance', figsize=(8,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance', figsize=(8,4))\n",
    "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster silhouette plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0], colors='yellowbrick')\n",
    "visualizer.fit(df_analysis)\n",
    "visualizer.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(7,5))\n",
    "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0], colors='yellowbrick', ax=axes)\n",
    "fig.fit(df_analysis)\n",
    "\n",
    "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight', dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
