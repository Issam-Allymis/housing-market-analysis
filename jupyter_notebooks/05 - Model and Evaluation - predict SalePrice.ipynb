{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Objectives\n",
    "* Fit and evaluate a classification model to predict if a prospect will churn or not.\n",
    "## Inputs\n",
    "* outputs/datasets/collection/house-price-2021.csv\n",
    "## Outputs\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* Data cleaning and Feature Engineering pipeline\n",
    "* Modeling pipeline\n",
    "* Feature importance plot\n",
    "---\n",
    "## Change working directory\n",
    "We need to change the working directory from its current folder to its parent folder\n",
    "\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory.\n",
    "\n",
    "    * os.path.dirname() gets the parent directory\n",
    "    * os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/collection/house-price-2021.csv\")\n",
    "      .drop(labels=['WoodDeckSF', 'EnclosedPorch'], axis=1)  \n",
    "                    \n",
    "  )\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: ML Pipeline with all data\n",
    "### ML pipeline for Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from feature_engine.selection import SmartCorrelatedSelection\n",
    "\n",
    "\n",
    "def PipelineDataCleaningAndFeatureEngineering():\n",
    "    # Converts Objects into Ints\n",
    "    df['BsmtExposure'] = df['BsmtExposure'].astype('category').cat.codes\n",
    "    df['BsmtFinType1'] = df['BsmtFinType1'].astype('category').cat.codes\n",
    "    df['GarageFinish'] = df['GarageFinish'].astype('category').cat.codes\n",
    "    df['KitchenQual'] = df['KitchenQual'].astype('category').cat.codes\n",
    "\n",
    "    \n",
    "    # Combine preprocessing with feature selection\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"feature_selection\", SelectFromModel(RandomForestRegressor())),\n",
    "        (\"SmartCorrelatedSelection\", SmartCorrelatedSelection(variables=None,\n",
    "                                                              method=\"pearson\", threshold=0.9, selection_method=\"variance\")),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base\n",
    "\n",
    "PipelineDataCleaningAndFeatureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Pipeline for Modelling and Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feat Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feat Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# ML algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def PipelineReg(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feat_selection\", SelectFromModel(model)),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Class for Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "\n",
    "            model = PipelineReg(self.models[key])\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, )\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score',\n",
    "                   'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns], self.grid_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['SalePrice'], axis=1),\n",
    "    df['SalePrice'],\n",
    "    test_size=0.2,\n",
    "    random_state=101,\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
    "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train, y_train) \n",
    "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Target Imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Train Set Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a random seed\n",
    "np.random.seed(101)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#subsampled_counts = y_train.value_counts().sample(n=20)  # Adjust the sample size as needed\n",
    "subsampled_counts = y_train.value_counts().sample(n=min(20, y_train.nunique()))\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "ax = subsampled_counts.plot(kind='bar', title='Train Set Target Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)# Rotate x-axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use algorithms that handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Define the NearMiss undersampler\n",
    "undersampler = NearMiss(version=1, n_neighbors=1)\n",
    "\n",
    "# Apply NearMiss undersampling to the dataset\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Train Set Target distribution after resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set a random seed\n",
    "np.random.seed(101)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#subsampled_counts = y_train.value_counts().sample(n=20)  # Adjust the sample size as needed\n",
    "subsampled_counts = y_train.value_counts().sample(n=min(20, y_train.nunique()))\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "ax = subsampled_counts.plot(kind='bar', title='Train Set Target Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)# Rotate x-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV - Sklearn\n",
    "#### Use standard hyperparameters to find most suitable algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=101),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=101),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=101),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=101),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=101),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=101),#, algorithm='SAMME'\n",
    "}\n",
    "\n",
    "params_search = {\n",
    "    \"LinearRegression\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick GridSearch CV - Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train,#_mapped,\n",
    "           scoring =  make_scorer(r2_score, ),\n",
    "           n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do an extensive search on the most suitable algorithm to find the best hyperparameter configuration.\n",
    "Define model and parameters, for Extensive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=101),\n",
    "}\n",
    "\n",
    "# Documentation to help on hyperparameter list:\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "# We will not conduct an extensive search, since the focus\n",
    "# is on how to combine all knowledge in an applied project.\n",
    "# In a workplace project, you may spend more time in this step\n",
    "params_search = {\n",
    "    \"XGBRegressor\": {\n",
    "        'model__learning_rate': [0.1, 0.01, 0.001],\n",
    "        'model__max_depth': [3, 5, 10],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensive GridSearch CV - Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, make_scorer\n",
    "search = HyperparameterOptimizationSearch(models=models_search, params=params_search)\n",
    "search.fit(X_train, y_train, #_mapped\n",
    "           scoring =  make_scorer(r2_score,),\n",
    "           n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best model name programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search_pipelines[best_model].best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the best clf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_reg = grid_search_pipelines[best_model].best_estimator_\n",
    "pipeline_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert X_train to a pandas DataFrame\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "\n",
    "# Now use the .tail() method\n",
    "X_train_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With the current model, we can assess with .features_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train is your NumPy array\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "\n",
    "# Now you can access the columns attribute\n",
    "columns = X_train_df.columns\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': columns[pipeline_reg['feat_selection'].get_support()],\n",
    "    'Importance': pipeline_reg['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "# re-assign best_features order\n",
    "best_features = df_feature_importance['Feature'].to_list()\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Pipeline on Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def reg_performance(X_train, y_train, X_test, y_test, pipeline):\n",
    "    print(\"#### Train Set #### \\n\")\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    print(\"Mean Squared Error (Train):\", mean_squared_error(y_train, y_train_pred))\n",
    "    print(\"R2 Score (Train):\", r2_score(y_train, y_train_pred))\n",
    "\n",
    "    print(\"\\n#### Test Set ####\\n\")\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    print(\"Mean Squared Error (Test):\", mean_squared_error(y_test, y_test_pred))\n",
    "    print(\"R2 Score (Test):\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress undefined metric warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: We cross check with metrics defined at ML business case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance(X_train=X_train, y_train=y_train,\n",
    "                 X_test=X_test, y_test=y_test,\n",
    "                 pipeline=pipeline_reg,\n",
    "                )\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Refit pipeline with best features\n",
    "### Refit ML Pipeline and Resampling\n",
    "In theory, a pipeline fitted **using only the most important features** should give the same result as the one fitted with **all variables and feature selection**.\n",
    "\n",
    "### Rewrite ML pipeline for Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Pipeline for DataCleaning And FeatureEngineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.encoding import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "def PipelineDataCleaningAndFeatureEngineering():\n",
    "    pipeline_base = Pipeline([\n",
    "\n",
    "        (\"Scaler\", StandardScaler()),  # Scale numerical features\n",
    "        (\"feature_selection\", SelectFromModel(RandomForestRegressor())),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite ML Pipeline for Modelling\n",
    "Function for Pipeline optmisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Optimization: Model\n",
    "def PipelineReg(model):\n",
    "    pipeline_base = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    return pipeline_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train Test Set, considering only with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop(['SalePrice'], axis=1)\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, y_train, y_train, y_test,  = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=101,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Target Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
    "\n",
    "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train, y_train)\n",
    "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Train Set Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "np.random.seed(101)\n",
    "\n",
    "# Plot bar chart\n",
    "counts = y_train.value_counts()\n",
    "subsampled_counts = counts.sample(n=min(20, len(counts)))  # Subsample the counts\n",
    "\n",
    "subsampled_counts.plot(kind='bar', title='Train Set Target Distribution')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Train Set Target Distribution')\n",
    "plt.xlabel('Target Value')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NearMiss to balance Train Set target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Define the NearMiss undersampler\n",
    "undersampler = NearMiss(version=1, n_neighbors=1)\n",
    "\n",
    "# Apply NearMiss undersampling to the dataset\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Train Set Target distribution after NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "np.random.seed(101)\n",
    "\n",
    "# Plot bar chart\n",
    "counts = y_train.value_counts()\n",
    "subsampled_counts = counts.sample(n=min(20, len(counts)))  # Subsample the counts\n",
    "\n",
    "subsampled_counts.plot(kind='bar', title='Train Set Target Distribution')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Train Set Target Distribution')\n",
    "plt.xlabel('Target Value')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV: Sklearn\n",
    "Using the most suitable model from the last section and its best hyperparameter configuration.\n",
    "\n",
    "We are using the same model from the last GridCV search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search   # XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the best parameters from the last GridCV search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to type in manually since the hyperparameter values have to be a list. The previous dictionary is not in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_search = {\n",
    "    'XGBRegressor': {\n",
    "        'model__learning_rate': [0.01],   # the value should be in []\n",
    "        'model__max_depth': [3],  # the value should be in []\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "quick_search = HyperparameterOptimizationSearch(\n",
    "    models=models_search, params=params_search)\n",
    "quick_search.fit(X_train, y_train,\n",
    "                 scoring=make_scorer(r2_score, ),\n",
    "                 n_jobs=-1, cv=3)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_summary, grid_search_pipelines = quick_search.score_summary(sort_by='mean_score')\n",
    "grid_search_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the best clf pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search_summary.iloc[0, 0]\n",
    "pipeline_reg = grid_search_pipelines[best_model].best_estimator_\n",
    "pipeline_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_features:\", best_features)\n",
    "print(\"feature_importances:\", pipeline_reg['model'].feature_importances_)\n",
    "print(\"Length of best_features:\", len(best_features))\n",
    "print(\"Length of feature_importances:\", len(pipeline_reg['model'].feature_importances_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "    'Feature': best_features,\n",
    "    'Importance': pipeline_reg['model'].feature_importances_})\n",
    "    .sort_values(by='Importance', ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Pipeline on Train and Test Sets\n",
    "Evaluation: We cross-check with metrics defined in the ML business case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_performance(X_train=X_train, y_train=y_train,\n",
    "                X_test=X_test, y_test=y_test,\n",
    "                pipeline=pipeline_reg,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Push files to Repo\n",
    "We will generate the following files\n",
    "\n",
    "* Train set\n",
    "* Test set\n",
    "* Data cleaning and Feature Engineering pipeline\n",
    "* Modeling pipeline\n",
    "* features importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_pipeline/predict_saleprice/{version}'\n",
    "\n",
    "try:\n",
    "    os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure when or how X_train became a numpy array. but I used the pd.DataFrame() method to convert it back into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert X_train to a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "X_train_df.to_csv(f\"{file_path}/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.to_csv(f\"{file_path}/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "* note that the variables are transformed already in X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert X_test to a DataFrame\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "# Now you can use DataFrame methods like head()\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_df.shape)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.to_csv(f\"{file_path}/X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipelines: Data Cleaning and Feat Eng pipeline and Modelling Pipeline\n",
    "We will save 2 pipelines:\n",
    "\n",
    "* Both should be used in conjunction to predict Live Data.\n",
    "* To predict on Train Set, Test Set we use only pipeline_reg, since the data is already processed.\n",
    "Pipeline responsible for Data Cleaning and Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data_cleaning_feat_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=pipeline_reg ,\n",
    "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
